# -*- coding: utf-8 -*-
"""mnistwithgan2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1N1-Dh7sPOHwMA_ZgJcIMQapADfgfyTFq
"""

import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt

!wget http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz
!wget http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz
!wget http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz
!wget http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz

!mkdir MNIST_Fashion
! cp *.gz MNIST_Fashion/
from tensorflow.examples.tutorials.mnist import input_data
mnist=input_data.read_data_sets("MNIST_Fashion/")

#training paramters
learning_rate=0.0002
batch_size=128
epochs=100000

#network parameters
image_dim=784
gen_hidd_dim=256
disc_hidd_dim=256
z_noise_dim=100

def xavier_init(shape):
  return tf.random_normal(shape=shape,stddev=1./tf.sqrt(shape[0]/2.0))

#define weight and bias dictionaries
weights={ "disc_H" : tf.Variable(xavier_init([image_dim,disc_hidd_dim])),
          "disc_final" : tf.Variable(xavier_init([disc_hidd_dim,1])),          
          "gen_H" : tf.Variable(xavier_init([z_noise_dim,gen_hidd_dim])),
          "gen_final" : tf.Variable(xavier_init([gen_hidd_dim,image_dim])),
        }
        
bias={ "disc_H" : tf.Variable(xavier_init([disc_hidd_dim])),
          "disc_final" : tf.Variable(xavier_init([1])),          
          "gen_H" : tf.Variable(xavier_init([gen_hidd_dim])),
          "gen_final" : tf.Variable(xavier_init([image_dim])),
        }

#interconnecting generator and discrimimator
#creating computational graph
#define discriminator function

def Discriminator(x):
  hidden_layer=tf.nn.relu(tf.add(tf.matmul(x,weights["disc_H"]), bias["disc_H"]))
  final_layer=tf.add(tf.matmul(hidden_layer,weights["disc_final"]),bias["disc_final"])
  disc_output=tf.nn.sigmoid(final_layer)
  return final_layer,disc_output
#define generator network
def Generator(x):
  hidden_layer=tf.nn.relu(tf.add(tf.matmul(x,weights["gen_H"]),bias["gen_H"]))
  final_layer=tf.add(tf.matmul(hidden_layer,weights["gen_final"]),bias["gen_final"])
  gen_output=tf.nn.sigmoid(final_layer)
  return gen_output
#define the placeholder for external input
z_input=tf.placeholder(tf.float32,shape=[None,z_noise_dim],name="input_noise")
x_input=tf.placeholder(tf.float32,shape=[None,image_dim],name="real_input")

#building the generator network
with tf.name_scope("Generator") as scope:
  output_Gen=Generator(z_input)
  
#building the discriminator network
with tf.name_scope("Discriminator") as scope:
  real_output1_Disc,real_output_Disc=Discriminator(x_input)
  fake_output1_Disc,fake_output_Disc=Discriminator(output_Gen)

#first kind of loss
with tf.name_scope("Discriminator_Loss") as scope:
  Discriminator_Loss=-tf.reduce_mean(tf.log(real_output_Disc + 0.0001) +tf.log(1.- fake_output_Disc +0.0001))
with tf.name_scope("Generator_Loss") as scope:
  Generator_Loss=-tf.reduce_mean(tf.log(fake_output_Disc + 0.0001))
  
#tensorboard Summary
Disc_loss_total=tf.summary.scalar("Disc_total_loss", Discriminator_Loss)
Gen_loss_total=tf.summary.scalar("Gen_loss",Generator_Loss)

# #second type of loss
# with tf.name_scope("Discriminator_Loss") as scope:
#   Disc_real_loss=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=real_output1_Disc,labels=tf.ones_like(real_output1_Disc)))
#   Disc_fake_loss=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_output1_Disc,labels=tf.ones_like(fake_output1_Disc)))
#   Discriminator_Loss=Disc_real_loss+Disc_fake_loss
# with tf.name_scope("Generator_Loss") as scope:
#   Generator_Loss=Disc_real_loss=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_output1_Disc,labels=tf.ones_like(fake_output1_Disc)))

#define the variables
Generator_var=(weights["gen_H"],weights["gen_final"],bias["gen_H"],bias["gen_final"])
Discriminator_var=(weights["disc_H"],weights["disc_final"],bias["disc_H"],bias["disc_final"])

#define the optimiser
with tf.name_scope("Optimizer_Discriminator") as scope:
  Discriminator_optimize=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(Discriminator_Loss,var_list=Discriminator_var)
with tf.name_scope("Optimizer_Generator") as scope:
  Generator_optimize=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(Generator_Loss,var_list=Generator_var)

#initialise the variable
init=tf.global_variables_initializer()

sess=tf.Session()
sess.run(init)
writer=tf.summary.FileWriter("./log",sess.graph)



for epoch in range(epochs):
  x_batch,_=mnist.train.next_batch(batch_size)
  #Generate noise to feed the discriminator
  z_noise=np.random.uniform(-1.,1.,size=[batch_size,z_noise_dim])
  _,Disc_loss_epoch=sess.run([Discriminator_optimize,Discriminator_Loss],feed_dict={x_input:x_batch, z_input:z_noise})
  _,Gen_loss_epoch=sess.run([Generator_optimize,Generator_Loss],feed_dict={ z_input:z_noise})
  #Running discriminator summary
  summary_Disc_Loss=sess.run(Disc_loss_total,feed_dict={x_input:X_batch, z_input:z_noise})
  #adding discriminator summary
  writer.add_summary(summary_Disc_Loss,epoch)
  #Running generator summary
  summary_Gen_Loss=sess.run(Gen_loss_total,feed_dict={z_input:z_noise})
  #adding generator summary
  writer.add_summary(summary_Gen_Loss,epoch)
  if epoch % 2000 == 0:
    print("steps:{},Generator Loss:{},Discriminator Loss:{}".format(epoch,Gen_loss_epoch,Disc_loss_epoch))

#testing
#generate image from noise using generator network
n=6
canvas=np.empty((28*n,28*n))
for i in range(n):
  #noise input
  z_noise=np.random.uniform(-1.,1.,size=[batch_size,z_noise_dim])
  #generate image from noise
  g=sess.run(output_Gen,feed_dict={z_input:z_noise})
  #reverse colors for better display
  g=-1 *(g-1)
  for j in range(n):
    #draw the generated digit
    canvas[i *28:(i+1) *28,j*28:(j+1)*28]=g[j].reshape([28,28])
plt.figure(figsize=(n,n))
plt.imshow(canvas,origin="upper",cmap="gray")
plt.show()